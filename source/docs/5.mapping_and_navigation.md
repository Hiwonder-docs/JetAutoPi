# 5. ROS2-Mapping & Navigation Courses

## 5.1 Mapping 

### 5.1.1 URDF Model

* **Introduction to URDF Model**

The Unified Robot Description Format (URDF) is an XML file format widely used in ROS (Robot Operating System) to comprehensively describe all components of a robot.

Robots are typically composed of multiple links and joints. A link is defined as a rigid object with certain physical properties, while a joint connects two links and constrains their relative motion.

By connecting links with joints and imposing motion restrictions, a kinematic model is formed. The URDF file specifies the relationships between joints and links, their inertial properties, geometric characteristics, and collision models.

* **Comparison between Xacro and URDF Model**

The URDF model serves as a description file for simple robot models, offering a clear and easily understandable structure. However, when it comes to describing complex robot structures, using URDF alone can result in lengthy and unclear descriptions.

To address this limitation, the xacro model extends the capabilities of URDF while maintaining its core features. The xacro format provides a more advanced approach to describe robot structures. It greatly improves code reusability and helps avoid excessive description length.

For instance, when describing the two legs of a humanoid robot, the URDF model would require separate descriptions for each leg. On the other hand, the xacro model allows for describing a single leg and reusing that description for the other leg, resulting in a more concise and efficient representation.

<p id="anchor_5_1_1_3"></p>

* **Basic Syntax of URDF Model**

(1) XML Basic Syntax

The URDF model is written using XML standard.

**Elements**:

An element can be defined as desired using the following formula:

```xml
<element>

</element>
```

**Properties**:

Properties are included within elements to define characteristics and parameters. Please refer to the following formula to define an element with properties:

```xml
<element

property_1="property value1"

property_2="property value2">

</element>
```

**Comments**:

Comments have no impact on the definition of other properties and elements. Please use the following formula to define a comment:

\<!-- comment content --\>

(2) Link

The Link element describes the visual and physical properties of the robot's rigid component. The following tags are commonly used to define the motion of a link:

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image2.png"  />

\<visual\>: Describe the appearance of the link, such as size, color and shape.

\<inertial\>: Describe the inertia parameters of the link, which will used in dynamics calculation.

\<collision\>: Describe the collision inertia property of the link

Each tag contains the corresponding child tag. The functions of the tags are listed below.

| **Tag**  |                         **Function**                         |
| :------: | :----------------------------------------------------------: |
|  origin  | Describe the pose of the link. It contains two parameters, including xyz and rpy. Xyz describes the pose of the link in the simulated map. Rpy describes the pose of the link in the simulated map. |
|   mess   |                Describe the mess of the link                 |
| inertia  | Describe the inertia of the link. As the inertia matrix is symmetrical, these six parameters need to be input, ixx, ixy, ixz, iyy, iyz and izz, as properties. These parameters can be calculated. |
| geometry | Describe the shape of the link. It uses mesh parameter to load texture file, and em\[ploys filename parameters to load the path for texture file. It has three child tags, namely box, cylinder and sphere. |
| material | Describe the material of the link. The parameter name is the required filed. The tag color can be used to change the color and transparency of the link. |

<p id="anchor_51133_joint"></p>

(3) Joint

The "**Joint**" tag describes the kinematic and dynamic properties of the robot's joints, including the joint's range of motion, target positions, and speed limitations. In terms of motion style, joints can be categorized into six types.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image3.png"  />

The following tags will be used to write joint motion.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image4.png"  />

\<parent_link\>: Parent link

\<child_link\>: Child link

\<calibration\>: Calibrate the joint angle

\<dynamics\>: Describes some physical properties of motion

\<limit\>: Describes some limitations of the motion

The function of each tag is listed below. Each tag involves one or several child tags.

|      **Tag**      |                         **Function**                         |
| :---------------: | :----------------------------------------------------------: |
|      origin       | Describe the pose of the parent link. It involves two parameters, including xyz and rpy. Both xyz and rpy describe the pose of the link in simulated map. |
|       axis        | Control the child link to rotate around any axis of the parent link. |
|       limit       | The motion of the child link is constrained using the lower and upper properties, which define the limits of rotation for the child link. The effort properties restrict the allowable force range applied during rotation (values: positive and negative; units: N). The velocity properties confine the rotational speed, measured in meters per second (m/s). |
|       mimic       |          Describe the relationship between joints.           |
| safety_controller | Describes the parameters of the safety controller used for protecting the joint motion of the robot. |

(4) Robot Tag

The complete top tags of a robot, including the \<link\> and \<joint\> tags, must be enclosed within the \<robot\> tag. The format is as follows:

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image5.png"  />

(5) gazebo Tag

This tag is used in conjunction with the Gazebo simulator. Within this tag, you can define simulation parameters and import Gazebo plugins, as well as specify Gazebo's physical properties, and more.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image6.png"  />

(6) Write Simple URDF Model

**Name the model of the robot**

To start writing the URDF model, we need to set the name of the robot following this format: "**\<robot name='robot model name'\>**". Lastly, input "**\</robot\>**" at the end to represent that the model is written successfully.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image7.png"  />

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image8.png"  />

(7) Set links

① To write the first link and use indentation to indicate that it is part of the currently set model. Set the name of the link using the following format: **\<link name="link name"\>**. Finally, conclude with "**\</link\>**" to indicate the successful completion of the link definition.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image9.png"  />

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image10.png"  />

② Write the link description and use indentation to indicate that it is part of the currently set link, and conclude with "**\</visual\>**".

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image11.png"  />

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image12.png"  />

③ The "**\<geometry\>**" tag is employed to define the shape of a link. Once the description is complete, include "**\</geometry\>**". Within the "**\<geometry\>**" tag, indentation is used to specify the detailed description of the link's shape. The following example demonstrates a link with a cylindrical shape: "**\<cylinder length='0.01' radius='0.2'/\>**". In this instance, "**length='0.01'**" signifies a length of 0.01 meters for the link, while "**radius='0.2'**" denotes a radius of 0.2 meters, resulting in a cylindrical shape.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image13.png"  />

④ The "**\<origin\>**" tag is utilized to specify the position of a link, with indentation used to indicate the detailed description of the link's position. The following example demonstrates the position of a link: "**\<origin rpy='0 0 0' xyz='0 0 0' /\>**". In this example, "**rpy**" represents the roll, pitch, and yaw angles of the link, while "**xyz**" represents the coordinates of the link's position. This particular example indicates that the link is positioned at the origin of the coordinate system.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image14.png"  />

⑤ The "**\<material\>**" tag is used to define the visual appearance of a link, with indentation used to specify the detailed description of the link's color. To start describing the color, include "**\<material\>**", and end with "**\</material\>**" when the description is complete. The following example demonstrates setting a link color to yellow: "**\<color rgba='1 1 0 1' /\>**". In this example, "**rgba='1 1 0 1'**" represents the color threshold for achieving a yellow color.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image15.png"  />

(8) Set joint

① To write the first joint, use indentation to indicate that the joint belongs to the current model being set. Then, specify the name and type of the joint as follows: "**\<joint name="joint name" type="joint type"\>**". Finally, include "**\</joint\>**" to indicate the completion of the joint definition.

:::{Note}

To learn about the type of the joint, please refer to "[**joint**](#anchor_51133_joint)".

:::

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image16.png"  />

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image17.png"  />

② Write the description section for the connection between the link and the joint. Use indentation to indicate that it is part of the currently defined joint. The parent parameter and child parameter should be set using the following format: "**\<parent link='parent link'/\>**", and "**\<child link='child link' /\>**". With the parent link serving as the pivot, the joint rotates the child link.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image18.png"  />

③ "**\<origin\>**" describes the position of the joint using indention. This example describes the position of the joint: "**\<origin xyz='0 0 0.1'/\>**". xyz is the coordinate of the joint.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image19.png"  />

④ "**\<axis\>**" describes the position of the joint adopting indention. "**\<axis xyz='0 0 1' /\>**" describes one posture of a joint. xyz specifies the pose of the joint.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image20.png"  />

⑤ "**\<limit\>**" imposes restrictions on the joint using indention. The below picture The "**\<limit\>**" tag is used to restrict the motion of a joint, with indentation indicating the specific description of the joint angle limitations. The following example describes a joint with a maximum force limit of 300 Newtons, an upper limit of 3.14 radians, and a lower limit of -3.14 radians. The settings are defined as follows: "**effort='joint force (N)', velocity='joint motion speed', lower='lower limit in radians', upper='upper limit in radians**'".

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image21.png"  />

⑥ "**\<dynamics\>**" describes the dynamics of the joint using indention. "**\<dynamics damping='50' friction='1' /\>**" describes dynamics parameters of a joint.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image22.png"  />

The complete codes are as below.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image23.png"  />

### 5.1.2 Explanation of ROS Robot URDF Model

* **Preparation**

To grasp the URDF model, check out "[**Basic Syntax of URDF Model**](#anchor_5_1_1_3)" for the key syntax. This part quickly breaks down the robot model code and its components.

* **Check Code of Robot Model**

(1) Start the robot, and access the robot system desktop using VNC.

(2) Click-on <img src="../_static/media/chapter_5/section_1/media/image24.png"  /> to open the command-line terminal.

(3) Run the following command to enable the app auto-start service, and hit Enter key.

```bash
~/.stop_ros.sh
```

(4) Enter the command and press Enter to access the startup program directory.

```bash
cd ~/ros2_ws/src/simulations/jetauto_description/urdf
```

(5) Enter the command to navigate to the robot simulation model folder.

```bash
vim jetauto.xacro
```

(6) Locate the code below:

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image28.png"  />

Several URDF models are combined to create a full robot:

|    **File Name**     |                          **Device**                          |
| :------------------: | :----------------------------------------------------------: |
|      materials       |                            Color                             |
|   inertial_matrix    |                        Inertia Matrix                        |
|       lidar_a1       |                           A1 Lidar                           |
|       lidar_g4       |                           G4 Lidar                           |
|     jetauto_arm      |                Exclusive to JetAutoPro Robot                 |
|     jetauto_car      |                        JetAuto Robot                         |
|         Imu          |                  Inertial Measurement Unit                   |
|     depth_camera     |                         Depth Camera                         |
|      usb_camera      |                          USB Camera                          |
|        common        |               Common Components or Attributes                |
|       connect        | Connectors, detailing the physical connections between robot components |
|       gripper        |                       Gripper Assembly                       |
|   arm.transmission   |              Robotic Arm Transmission Structure              |
| gripper.transmission |                Gripper Transmission Structure                |

* **Brief Analysis of Robot's Main Body Model**

Open a new command prompt and enter the following command to open the robot model file, which contains descriptions of each part of the robot:

```bash
vim jetauto_car.urdf.xacro
```

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image30.png"  />

This is the start of the URDF file. It specifies the XML version and encoding while defining a robot model named `jetauto`. The `xmlns:xacro` namespace is also included to facilitate the generation of URDF using Xacro macros. This line of code defines a Xacro attribute called `M_PI` and assigns it the value of the mathematical constant π.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image31.png" style="width:100%;" />

In this section, a link named `base_footprint` is defined as the robot's chassis.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image32.png" style="width:100%;" />

Various characteristics of the robot such as mass, width, height, and depth are specified.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image33.png"  />

Next, a joint called  `base_joint`  is defined with a type of "**fixed**", indicating it's a stationary joint. It connects the parent link `base_footprint` with the child link "**base_link**".

The joint's position (origin) is determined using an xyz attribute.

```xml
<link name="base_footprint"/>

<joint name="base_joint" type="fixed">

 <parent link="base_footprint"/>

 <child link="base_link"/>

 <origin xyz="0.0 0.0 0.005" rpy="0 0 0"/>

</joint>
```

The following code is an XML snippet that defines a link in a robot model. Let's break down and analyze its structure and purpose.

The code begins with the \<link\> tag, which defines a link within the robot model. This link is named `base_green_link.` Inside the \<link\> tag, there are three sections: \<inertial\>, \<visual\>, and \<collision\>.

The \<inertial\> section defines the link's inertial properties, such as mass and inertia. It contains an \<origin\> tag that specifies the position and orientation of the inertial frame relative to the link frame. The \<mass\> tag specifies the link's mass, while the \<inertia\> tag defines the inertia matrix around the link's principal axes.

The \<visual\> section defines the visual representation of the link. It also contains an \<origin\> tag that specifies the position and orientation of the visual frame relative to the link frame. The \<geometry\> tag defines the shape of the visual representation, which, in this case, is a mesh. The \<mesh\> tag specifies the filename of the mesh file that represents the visual appearance of the link. Lastly, the \<material\> tag defines the color or texture of the visual representation, here represented by a material called 'green.'

The \<collision\> section defines the collision properties of the link. It is similar to the \<visual\> section but is used for collision detection rather than visualization. It includes an \<origin\> tag and a \<geometry\> tag that define the position, orientation, and shape of the collision representation.

Overall, this code snippet defines a link in the robot model, including its inertial properties, visual representation, and collision attributes. In simulation or visualization environments, the mesh files specified in the \<visual\> and \<collision\> sections are used for visual representation and collision detection with the link.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image34.png"  />

The following code describes a joint named `base_green_joint` of type `fixed,` which means it is a fixed joint. The joint's parent link is named `base_link,` and its child link is named  `base_green_joint.`  The joint's coordinate origin is at (0, 0, 0), the Euler angles (rpy) are (0, 0, 0), and the joint has no defined axis.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image35.png" style="width:100%;" />

Next, let's take a look at the description of the link:

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image36.png"  />

The above code describes a link named `wheel_right_front_link`, which contains information about its inertia (inertial), visual representation (visual), and collision shape (collision).

The inertial part describes the link's mass and inertia matrix. The mass is 0.124188560741815, and specific values are provided for the inertia matrix components.

The visual part details the link's appearance, defined by a three-dimensional model (mesh) with the filename `package://jetauto_description/meshes/wheel_left_front_link.stl`. The link uses a material named `black`.

The collision part specifies the link's collision shape, also defined by a three-dimensional model (mesh) with the same filename as the visual part.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image37.png"  />

The following code snippet is an XML fragment used to define a joint and a link in a robot model. The joint definition includes:

**Name:**  `wheel_left_back_joint`

**Type:**  `fixed` (indicating a fixed joint that does not allow movement)

**Origin:** Specifies the position and orientation of the joint relative to the parent link ("**base_link**").

**Parent Link:** Specifies the link to which the joint is attached.

**Child Link:** Specifies the link that the joint connects to.

**Axis:** Specifies the rotation axis of the joint. In this case, the axis is set to (0, 0, 0), indicating that it is a fixed joint with no rotation capability.

The link definition includes:

**Name:** `wheel_left_back_link`

**Inertia:** Specifies the inertial properties of the link, including mass, center of mass, and moment of inertia.

**Visual:** Specifies the visual representation of the link, including its position, orientation, geometry (mesh), and material.

**Collision:** Specifies the collision properties of the link, including its position, orientation, and geometry (mesh).

Another link definition is provided with the name

**Name:** `wheel_right_front_link`

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image38.png"  />

**Inertia:** Specifies the inertial properties of the link, including mass and inertia.

**Visualization:** Specifies the visual representation of the link, including its position, orientation, geometry (mesh), and material.

**Collision:** Specifies the collision properties of the link, including its position, orientation, and geometry (mesh).

Detailed Explanation:

Inertial Properties (inertial): Defines the mass and inertia matrix of the link. In this example, the mass of the link is 0.124186629923608, and the individual components of the inertia matrix (ixx, ixy, ixz, iyy, iyz, izz) are also specified with precise values.

Visual Properties (visual): Defines the visual representation of the link. Here, the link's visualization uses a mesh with the file name `package://jetauto_description/meshes/wheel_right_front_link.stl`. Additionally, a material named `black` is applied for visualization.

Collision Properties (collision): Defines the collision properties of the link. In this case, the collision properties are identical to the visual properties, using the same mesh.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image39.png"  />

The following code defines a fixed joint that connects two links: `base_link` and `wheel_right_front_link`. The joint’s initial position and rotation are specified using the \<origin\> tag.

**name=wheel_right_front_joint:** This attribute sets the joint’s name to "**wheel_right_front_joint**".

**type=fixed:** This attribute indicates that the joint is of type `fixed`, meaning it is stationary and does not allow movement.

<origin\> tag: This tag specifies the joint’s initial position and rotation. The joint's xyz coordinates are set to "**-0.10658 0.091851 -0.065487**", and the rpy (roll, pitch, yaw) rotation angles are set to "**0 0 3.1416**".

**<parent\> tag:** This tag designates `base_link` as the parent link of the joint, meaning the joint is connected to `base_link`.

**<child\> tag:** This tag designates `wheel_right_front_link` as the child link of the joint, meaning the joint is connected to `wheel_right_front_link`.

**<axis\> tag:** This tag defines the joint’s axis. In this case, the axis is set to (0, 0, 0), indicating that the joint has no specific axis of rotation.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image40.png"  />

The following describes a link named `"wheel_right_back_link"`, including its inertial, visual, and collision properties. The link's geometry is defined using a mesh file, and its visual properties also specify a black material.

`name="wheel_right_back_link"`: This attribute sets the name of the link to `"wheel_right_back_link"`.

`<inertia>` tag: Defines the inertial properties of the link, including its mass and inertia matrix.

`<origin>` tag: Specifies the position and rotation of the link’s inertia origin. Specifically, the xyz coordinates are \`"**0.21333 0.20224 0.00032742**"\`, and the rpy (roll, pitch, yaw) rotation angles are \`"**0 0 0**"\`.

`<mass>` tag: Defines the mass of the link, which is \`"**0.13231**"\`.

`<inertia>` tag: Specifies the inertia matrix of the link, including the values for ixx, ixy, ixz, iyy, iyz, and izz.

`<visual>` tag: Defines the visual properties of the link, including its appearance and material.

`<origin>` tag: Specifies the position and rotation of the link’s visual origin. The xyz coordinates are \`"**0 0 0**"\`, and the rpy rotation angles are \`"**0 0 0**"\`.

`<geometry>` tag: Defines the geometry of the link, including a mesh file.

`<mesh>` tag: Specifies the mesh file used for the link’s geometry, located at \`"**package://jetauto_description/meshes/wheel_right_back_link.stl**"\`.

`<material>` tag: Defines the material of the link, specified as \`"**black**"\`.

`<collision>` tag: Specifies the collision properties of the link, including its geometry.

`<origin>` tag: Specifies the position and rotation of the link’s collision origin. The xyz coordinates are \`"**0 0 0**"\`, and the rpy rotation angles are \`"**0 0 0**"\`.

`<geometry>` tag: Defines the geometry of the link, including a mesh file.

`<mesh>` tag: Specifies the mesh file used for the link’s geometry, located at \`"**package://jetauto_description/meshes/wheel_right_back_link.stl**"\`.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image41.png"  />

The following code snippet describes a joint named "**wheel_right_back_joint**":

`\<joint\>`: This is the opening tag for the joint element, defining a joint with **name='wheel_right_back_joint'**.

`type="fixed"`: Specifies that the joint is of type "**fixed**", meaning it is a stationary joint that does not permit movement.

`\<origin\>`: This tag defines the joint's origin, including its position and orientation.

`xyz="0.10675 0.091848 -0.065821"`: Indicates the position of the joint's origin in 3D space as (0.10675, 0.091848, -0.065821).

`rpy="0 0 3.1416"`: Represents the orientation of the joint's origin using Euler angles, with roll, pitch, and yaw set to 0, 0, and 3.1416 respectively.

\<parent\>: This tag specifies the joint’s parent link.

`link="base_link"`: Identifies the parent link of this joint as "**base_link**".

`\<child\>`: This tag specifies the joint’s child link.

`link="wheel_right_back_link"`: Identifies the child link of this joint as "**wheel_right_back_link**".

\<axis\>: This tag defines the axis of the joint.

`xyz="0 0 0"`: Sets the joint’s axis to (0, 0, 0), indicating that the joint does not have a defined axis of rotation.

`\</joint\>`: This is the closing tag for the joint element.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image42.png"  />

### 5.1.3 Principles of SLAM Mapping

* **Introduction to SLAM**

SLAM stands for Simultaneous Localization and Mapping.

Localization involves determining the pose of a robot in a coordinate system,, The origin of orientation of the coordinate system can be obtained from the first keyframe, existing global maps, landmarks or GPS data.

Mapping involves creating a map of the surrounding environment perceived by the robot. The basic geometric elements of the map are points. The main purpose of the map is for localization and navigation. Navigation can be divided into guidance and control. Guidance includes global planning and local planning, while control involves controlling the robot's motion after the planning is done.

* **SLAM Mapping Principle**

SLAM mapping mainly consists of the following three processes:

(1) Preprocessing: Optimizing the raw data from the radar point cloud, filtering out problematic data or performing filtering. Using laser as a signal source, pulses of laser emitted by the laser are directed at surrounding obstacles, causing scattering.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image43.jpeg"   />

Some of the light waves will reflect back to the receiver of the lidar, and then, according to the principle of laser ranging, the distance from the lidar to the target point can be obtained.

Regarding point clouds: In simple terms, the surrounding environment information obtained by lidar is called a point cloud. It reflects a portion of what the 'eyes' of the robot can see in the environment where it is located. The object information collected presents a series of scattered, accurate angle, and distance information.

(2) Matching: Matching the point cloud data of the current local environment with the established map to find the corresponding position.

(3) Map Fusion: Integrating new round data from the lidar into the original map, ultimately completing the map update.

* **Notes**

(1) Begin the mapping process by positioning the robot in front of a straight wall or within an enclosed box. This enhances the Lidar's capacity to capture a higher density of scanning points.

(2) Initiate a 360-degree scan of the environment using the Lidar to ensure a comprehensive survey of the surroundings. This step is crucial to guarantee the accuracy and completeness of the resulting map.

(3) For larger areas, it's recommended to complete a full mapping loop before focusing on scanning smaller environmental details. This approach enhances the overall efficiency and precision of the mapping process.

* **Judge Mapping Result**

Finally, assess the robot's navigation process against the following criteria once the mapping is complete:

(1) Ensure that the edges of obstacles within the map are distinctly defined.

(2) Check for any disparities between the map and the actual environment, such as the presence of closed loops or inconsistencies.

(3) Verify the absence of gray areas within the robot's motion area, indicating areas that haven't been adequately scanned.

(4) Confirm that the map doesn't incorporate obstacles that won't exist during subsequent localization.

(5) Validate the map's coverage of the entire extent of the robot's motion area.

<p id="anchor_5_1_4"></p>

### 5.1.4 slam_toolbox Mapping Algorithm

* **Mapping Definition**

Slam Toolbox software package combines information from laser rangefinders in the form of LaserScan messages and performs TF transformation from odom-\> base link to create a two-dimensional map of space. This software package allows for fully serialized reloadable data and pose graphs of SLAM maps, used for continuous mapping, localization, merging, or other operations. It allows Slam Toolbox to operate in synchronous (i.e., processing all valid sensor measurements regardless of delay) and asynchronous (i.e., processing valid sensor measurements whenever possible) modes.

ROS replaces functionalities like gmapping, cartographer, karto, hector, providing comprehensive SLAM functionality built upon the powerful scan matcher at the core of Karto, widely used and accelerated for this package. It also introduces a new optimization plugin based on Google Ceres. Additionally, it introduces a new localization method called 'elastic pose-graph localization,' which takes measured sliding windows and adds them to the graph for optimization and refinement. This allows for tracking changes in local features of the environment instead of considering them as biases, and removes these redundant nodes when leaving an area without affecting the long-term map.

Slam Toolbox is a suite of tools for 2D Slam, including:

① Mapping and Saving Maps (pgm Files): Create and save maps in the pgm file format.

② Refining and Continuing Mapping: Refine existing maps, remap, or extend mapping on previously saved maps.

③ Long-Term Mapping: Continue mapping with a previously saved map, removing irrelevant data from new laser point clouds.

④ Localization Optimization on Existing Maps: Optimize localization using an existing map. Alternatively, use "**Laser Odometry**" mode for localization without mapping.

⑤ Synchronous and Asynchronous Mapping: Supports both synchronous and asynchronous mapping.

⑥ Dynamic Map Merging: Merge maps dynamically.

⑦ Plugin-Based Optimization Solver: Includes a new optimization plugin based on Google Ceres.

⑧ Interactive RVIZ Plugin: Provides an interactive RVIZ plugin for map manipulation.

⑨ RVIZ Graphical Tools: Offers graphical tools in RVIZ for managing nodes and connections during the mapping process.

⑩ Map Serialization and Lossless Data Storage: Supports map serialization and ensures lossless data storage.

**KARTO:**

Karto_SLAM is based on graph optimization techniques, utilizing highly efficient and non-iterative Cholesky decomposition to solve sparse systems. In this method, the map is represented as a graph where each node represents a specific position in the robot's trajectory along with its associated sensor measurements. As new nodes are added, the computations are updated accordingly.

The ROS version of Karto_SLAM incorporates Sparse Pose Adjustment (SPA), which is linked to scan matching and loop closure detection. Although including more landmarks increases memory requirements, graph optimization methods like Karto_SLAM are particularly advantageous in large environments. This is because they only represent the robot's poses in the graph and determine the map after computing these poses.

The algorithmic framework of Karto_SLAM is shown in the diagram below:

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image44.png"   />

From the diagram above, it can be seen that the process is quite straightforward. The SLAM system operates with a traditional soft real-time mechanism: each new frame of data is processed as it arrives and then returned.

KartoSLAM Source Code and Wiki Address:

**KartoSLAM ROS Wiki:** http://wiki.ros.org/slam_karto

**slam_karto software package:** https://github.com/ros-perception/slam_karto

**open_karto open-source algorithm:** https://github.com/ros-perception/open_karto

*  **Mapping Operation Steps**

ROS2 Mapping and Navigation: In ROS2, mapping and navigation are conducted by connecting the robot to a virtual machine within the same local area network (LAN).

* **Install Virtual Machine Software and Import the Virtual Machine**

(1) **Extraction Path:** The installation files are located in "[**Appendix->VMware**]()". Double-click the installation file to start the installation process.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image45.png"   />

(2) Click-on <img src="../_static/media/chapter_5/section_1/media/image46.png"  /> to start the virtual machine.

(3) Access the virtual machine interface and click "**Open a Virtual Machine**".

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image47.png"  />

(4) Select the extracted ubuntu_ros2_humble folder (located in [**Appendix->ubuntu_ros2_humble**]()), choose the Ubuntu 22.04 ROS2.ovf file, and click **"Open"**.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image48.png"  />

(5) Enter your desired virtual machine name, choose your path, and click **"Import"**.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image49.png"  />

(6) Importing in progress, displaying the import status.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image50.png"  />

(7) After the import is complete, follow the prompts to finish the installation process.

* **Copying Robot Files to the Virtual Machine**

(1) Exporting Files from the Robot

① Start the robot, and connect the robot to VNC remote control software.

② Click-on <img src="../_static/media/chapter_5/section_1/media/image24.png"  /> to initiate the ROS2 command-line terminal.

③ Execute the command below to disable the app auto-start service.

```bash
~/.stop_ros.sh
```

④ Enter the command to compress the three directories—navigation, slam, and simulations—within the function package into a single archive. Then, copy this archive to the shared directory tmp.

```bash
tar -cvf ~/share/tmp/src.tar -C ~/ros2_ws/src ./navigation ./slam ./simulations
```

⑤ Next, enter the command to copy the .typerc file to the shared directory tmp.

```bash
cp ~/ros2_ws/.typerc ~/share/tmp
```

⑥ Use the SSH remote connection tool MobaXterm to drag the src.tar and .typerc files from the /home/pi/docker/tmp folder on the Raspberry Pi system image to your local PC.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image54.png"  />

If you can't find the **.typerc** file, you may need to click the **"Show Hidden Files"** button to display hidden files.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image55.png"  />

For detailed instructions on how to connect using the SSH remote connection tool MobaXterm, please refer to "[**1.Quick Start Guide(JetAuto User Manual)-\> 1.6 Development Environment Setup and Configuration**](https://docs.hiwonder.com/projects/JetAutoPi/en/latest/docs/1.quick_start_guide.html#development-environment-setup-and-configuration)".

(2) Import Files into Virtual Machine

① Click-on <img src="../_static/media/chapter_5/section_1/media/image56.png"  /> to navigate to the home directory.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image57.png"  />

② Drag and drop the **.typerc** and **src.tar** files exported from the robot into the virtual machine.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image58.png"  />

* **Create and Compile Workspace**

(1) Click-on <img src="../_static/media/chapter_5/section_1/media/image59.png"  /> to start the command-line terminal on the virtual machine.

(2) Enter the command to create the **ros2_ws/src** directory.

```bash
mkdir -p ros2_ws/src
```

(3) Extract **src.tar** to the current **home/ubuntu** directory.

```bash
tar -xvf src.tar -C ~/ros2_ws/src
```

(4) Move the .typerc file to the ros2_ws directory.

```bash
mv .typerc ~/ros2_ws
```

(5) Enter the ros2_ws directory.

```bash
cd ~/ros2_ws
```

(6) Enter the command to check if the .typerc file has been moved to the ros2_ws directory.

```bash
ls -a
```

(7) Execute the following command to compile the workspace.

```bash
colcon build
```

(8) Modify the .bashrc file by entering the following command:

```bash
gedit ~/.bashrc
```

Copy and add the following content to the .bashrc file:

```bash
source /home/ubuntu/ros2_ws/.typerc

source /home/ubuntu/ros2_ws/install/setup.bash
```

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image67.png"  />

(9) After editing, save and exit by pressing **Ctrl + S** or clicking the **"Save"** button in the upper-right corner.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image68.png"  />

(10) Run the following command to refresh the environment configuration.

```bash
source ~/.bashrc
```

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image69.png"  />

* **Configuring the Robot for Local Network Mode**

(1) Click on <img src="../_static/media/chapter_5/section_1/media/image70.png"  /> to open the command line terminal. Then, enter the command to navigate to the wifi_manager directory.

```bash
cd ~/wifi_manager
```

(2) Use the command to edit the WiFi configuration file. Switch it to local network mode and enter your own WiFi name and password.

```bash
gedit wifi_manager/wifi_conf.py
```

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image73.png" style="width:900px" />

(3) After making your changes, save the file by pressing \`Ctrl + S\` or by clicking the \`Save\` button in the top-right corner of the editor, then close the editor.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image74.png" style="width:900px" />

(4) Restart the network service with the following command, or shut down and restart the robot. It is recommended to shut down and restart the robot. Check the IP address on the robot's OLED screen and connect to it:

```bash
sudo systemctl restart wifi.service
```

(5) Make sure the virtual machine and the robot are connected to the same local network. The IP addresses should be in the same subnet.

Virtual Machine:

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image76.png"  />

Robot:

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image77.png" style="width:900px" />

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image78.png" style="width:900px" />

(6) After restarting, click on <img src="../_static/media/chapter_5/section_1/media/image24.png"  /> to open the ROS 2 command line terminal on the robot. Ensure that the `ROS_DOMAIN_ID` is set to 0, matching the virtual machine:

Robot:

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image79.png"  />

Virtual machine: Click-on <img src="../_static/media/chapter_5/section_1/media/image59.png"  /> to access the virtual machine command-line terminal.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image80.png" style="width:900px" />

* **SLAM Mapping Operation Steps**

(1) Operation on the robot end

① Click-on <img src="../_static/media/chapter_5/section_1/media/image24.png"  /> to start the command-line terminal.

② Execute the command to disable the app auto-start service.

```bash
~/.stop_ros.sh
```

③ Enter the command to start mapping:

```bash
ros2 launch slam slam.launch.py
```

(2) Operations on virtual machine

① Click-on <img src="../_static/media/chapter_5/section_1/media/image59.png"  /> to start the command-line terminal in the virtual machine system.

② Execute the following command to launch the RViz tool and display the map-building results.

```bash
ros2 launch slam rviz_slam.launch.py
```

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image83.png"  />

(3) Enable Keyboard Control for the Robot

① Click <img src="../_static/media/chapter_5/section_1/media/image24.png"  /> to open the command line terminal.

② Enter the following command to start the keyboard control node and press **Enter** key:

```bash
ros2 launch peripherals teleop_key_control.launch.py
```

If the terminal displays the confirmation message shown below, the keyboard control service has started successfully.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image85.png"  />

③ Use your keyboard to move the robot around the area to create a more complete map. The table below shows the keyboard keys and their functions for controlling the robot’s movement.

| **Key** |                       **Robot Action**                       |
| :-----: | :----------------------------------------------------------: |
|    W    | Short press to switch to forward state and continuously move forward |
|    S    | Short press to switch to backward state and continuously move backward |
|    A    | Long press to interrupt the forward or backward state and turn left |
|    D    | Long press to interrupt the forward or backward state and turn right |

④ When controlling the robot's movement with the keyboard to map, it's advisable to reduce the robot's movement speed appropriately. The slower the robot's speed, the smaller the odometry relative error, leading to better mapping results. As the robot moves, the map displayed in RVIZ will continuously expand until the entire environment scene's map construction is completed.

* **Save Map**

① Click-on <img src="../_static/media/chapter_5/section_1/media/image24.png"  /> to initiate the command-line terminal.

② Run the following command and hit Enter key:

```bash
cd ~/ros2_ws/src/slam/maps && ros2 run nav2_map_server map_saver_cli -f "map_01" --ros-args -p map_subscribe_transient_local:=true
```

* **Outcome Optimization**

If you want to achieve more precise mapping results, you can optimize the odometry. The robot relies on odometry for mapping, which in turn depends on the IMU.

The robot already has calibrated IMU data loaded, allowing it to perform mapping and navigation functions effectively. However, we can further calibrate the IMU to obtain higher precision. For methods and steps on calibrating the IMU, please refer to the section [**2. ROS2-Motion Control Course -> 2.2 Motion Control -> 2.2.1 IMU, Linear Velocity, and Angular Velocity Calibration**](https://docs.hiwonder.com/projects/JetAutoPi/en/latest/docs/2.motion_control.html#imu-linear-velocity-and-angular-velocity-calibration).

* **Parameter Description**

The parameter file can be found at the path `ros2_ws/src/slam/config/slam.yaml.`

For more detailed information about the parameters, please refer to the official documentation: **https://wiki.ros.org/slam_toolbox**.

* **Launch File Analysis**

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image87.png"   />

The launch file is located at: [/home/ubuntu/ros2_ws/src/slam/launch/slam.launch.py](../_static/source_code/slam.launch.zip)

- **Import Library**

The launch library can be explored in detail in the official ROS documentation: [https://docs.ros.org/en/humble/How-To-Guides/Launching-composable-nodes](https://docs.ros.org/en/humble/How-To-Guides/Launching-composable-nodes)

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image88.png"  />

- **Set the Storage Path**

Use the `get_package_share_directory` function to obtain the path of the slam package.

{lineno-start=30}

```python
    if compiled == 'True':
        slam_package_path = get_package_share_directory('slam')
    else:
        slam_package_path = '/home/ubuntu/ros2_ws/src/slam'
```

- **Initiate Other Launch File**

{lineno-start=35}

```python
    base_launch = IncludeLaunchDescription(
        PythonLaunchDescriptionSource(
            os.path.join(slam_package_path, 'launch/include/robot.launch.py')),
        launch_arguments={
            'sim': sim,
            'master_name': master_name,
            'robot_name': robot_name
        }.items(),
    )

    slam_launch = IncludeLaunchDescription(
        PythonLaunchDescriptionSource(
            os.path.join(slam_package_path, 'launch/include/slam_base.launch.py')),
        launch_arguments={
            'use_sim_time': use_sim_time,
            'map_frame': map_frame,
            'odom_frame': odom_frame,
            'base_frame': base_frame,
            'scan_topic': '{}/scan'.format(frame_prefix),
            'enable_save': enable_save
        }.items(),
    )

    if slam_method == 'slam_toolbox':
        bringup_launch = GroupAction(
         actions=[
             PushRosNamespace(robot_name),
             base_launch,
             TimerAction(
                 period=5.0,  # 延时等待其它节点启动好
                 actions=[slam_launch],
             ),
          ]
        )
```
  
`base_launch`: Launch for hardware initialization

`slam_launch`: Launch for basic mapping

`bringup_launch`: Launch for initial pose setup

### 5.1.5 RTAB-VSLAM 3D Vision Mapping & Navigation

* **RTAB-VSLAM Description**

RTAB-VSLAM is a appearance-based real-time 3D mapping system, it's an open-source library that achieves loop closure detection through memory management methods. It limits the size of the map to ensure that loop closure detection is always processed within a fixed time limit, thus meeting the requirements for long-term and large-scale environment online mapping.

* **RTAB-VSLAM Working Principle**

RTAB-VSLAM 3D mapping employs feature mapping, offering the advantage of rich feature points in general scenes, good scene adaptability, and the ability to use feature points for localization. However, it has drawbacks, such as a time-consuming feature point calculation method, limited information usage leading to loss of image details, diminished effectiveness in weak-texture areas, and susceptibility to feature point matching errors, impacting results significantly.

After extracting features from images, the algorithm proceeds to match features at different timestamps, leading to loop detection. Upon completion of matching, data is categorized into long-term memory and short-term memory. Long-term memory data is utilized for matching future data, while short-term memory data is employed for matching current time-continuous data.

During the operation of the RTAB-VSLAM algorithm, it initially uses short-term memory data to update positioning points and build maps. As data from a specific future timestamp matches long-term memory data, the corresponding long-term memory data is integrated into short-term memory data for updating positioning and map construction.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image91.png"   />

RTAB-VSLAM software package link: <https://github.com/introlab/rtabmap>

* **RTAB-VSLAM 3D Mapping Instructions**

(1) Robot Operation

① Click on <img src="../_static/media/chapter_5/section_1/media/image24.png"  /> on the system desktop to open the command-line terminal.

② Execute the following command to terminate the app auto-start service.

```bash
~/.stop_ros.sh
```

③ Execute the command to start mapping:

```bash
ros2 launch slam rtabmap_slam.launch.py
```

(2) Operations on Virtual Machine

① Click-on <img  src="../_static/media/chapter_5/section_1/media/image59.png"  /> to start the command-line terminal.

② Run the following command to initiate the rviz tool tp display the mapping result.

```bash
ros2 launch slam rviz_rtabmap.launch.py
```

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image94.png"  />

(3) Enable Keyboard Control

① Click-on <img src="../_static/media/chapter_5/section_1/media/image24.png"  /> to launch a new command-line terminal.

② Enter the command to activate the keyboard control node and press Enter.

```bash
ros2 launch peripherals teleop_key_control.launch.py
```

If the prompt shown in the image below appears, it means the keyboard control service has started successfully.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image85.png"  />

③ Move the robot around the current space to build a more complete map. The table below shows the keyboard keys and their functions for controlling the robot's movement:

| **Button** |                      **Robot’s Action**                      |
| :--------: | :----------------------------------------------------------: |
|     W      | Short press: Engage forward movement; the robot will move continuously in the forward direction |
|     S      | Short press: Engage backward movement; the robot will move continuously in the backward direction |
|     A      | Long press: Stop forward or backward movement and initiate a left turn |
|     D      | Long press: Stop forward or backward movement and initiate a right turn |

When controlling the robot for mapping using the keyboard, it's best to reduce the robot's speed. Slower movement results in smaller odometry errors and improves mapping accuracy. As the robot moves, RViz will continuously update the map until the entire environment is fully covered.

* **Map Saving**

After mapping is completed, you can use the shortcut "**Ctrl+C**" in each command-line terminal window to close the currently running program.

:::{Note}

For 3D mapping, there's no need to manually save the map. When you use "Ctrl+C" to close the mapping command, the map will be automatically saved.

:::

* **lanunch File Analysis**

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image95.png"   />

The launch file is located at: [/home/ubuntu/ros2_ws/src/slam/launch/rtabmap_slam.launch.py](../_static/source_code/rtabmap_slam.launch.zip)

(1) Import Library

You can refer to the ROS official documentation for detailed analysis of the launch library: "[**https://docs.ros.org/en/humble/How-To-Guides/Launching-composable-nodes.html**](https://docs.ros.org/en/humble/How-To-Guides/Launching-composable-nodes.html)"

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image96.png" style="width:900px" />

(2) Set the Storage Path

Use `get_package_share_directory` to obtain the path of the slam package.

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image97.png" style="width:900px"  />

(3) Initiate Other Launch File

<img class="common_img" src="../_static/media/chapter_5/section_1/media/image98.png"  />

`base_launch`: Launch for hardware initialization

`slam_launch`: Basic mapping launch

`rtabmap_launch`: RTAB mapping launch

`bringup_launch`: Initial pose launch

## 5.2 Navigation Tutorial

### 5.2.1 ROS Robot Autonomous Navigation

* **Description**

Autonomous navigation involves guiding a device along a predefined route from one point to another. It finds application in various domains:

Land Applications: Including autonomous vehicle navigation, vehicle tracking and monitoring, intelligent vehicle information systems, Internet of Vehicles applications, railway operation monitoring, etc.

Navigation Applications: Encompassing ocean transportation, inland waterway shipping, ship berthing and docking, etc.

Aviation Applications: Such as route navigation, airport surface monitoring, precision approach, etc.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image2.png"  />

ROS (Robot Operating System) follows the principle of leveraging existing solutions and provides a comprehensive set of navigation-related function packages. These packages offer universal implementations for robot navigation, sparing developers from dealing with complex low-level tasks like navigation algorithms and hardware interactions. Managed and maintained by professional R&D personnel, these implementations allow developers to focus on higher-level functions. To utilize the navigation function, developers simply need to configure relevant parameters for their robots in the provided configuration files. Custom requirements can also be accommodated through secondary development of existing packages, enhancing research and development efficiency and expediting product deployment.

In summary, ROS's navigation function package set offers several advantages for developers. Developed and maintained by a professional team, these packages provide stable and comprehensive functionality, allowing developers to concentrate on upper-layer functions, thus streamlining development processes.

The ROS navigation function package, ros-navigation, is a crucial component within the ROS ecosystem, serving as the foundation for many autonomous mobile robot navigation functions. Subsequent sections will delve into its operation, principle analysis, installation, and usage.

* **Package Explanation**

This section will provide a detailed analysis and explanation of the navigation function package, including the usage of parameters.

(1) Principle Structure Framework Explanation

The Nav2 project inherits and promotes the spirit of the ROS Navigation Stack. The project strives to safely navigate mobile robots from point A to point B. Nav2 can also be applied to other applications, including robot navigation tasks such as dynamic point tracking, which involves dynamic path planning, velocity calculation, obstacle avoidance, and recovery behaviors.

Nav2 utilizes behavior trees to invoke modular servers to accomplish an action. These actions can involve path computation, control forces, recovery, or any other navigation-related operation. These are achieved through independent nodes communicating with ROS Action servers and the behavior tree (BT). You can refer to the diagram below for a preliminary understanding of Nav2's architecture:

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image3.png"   />

The architecture diagram above can be further explained as one major component and three minor components, totaling four services.

**Major:**

BT Navigator Server: This service organizes and invokes the following three minor services.

**Minor:**

**① Planner Server:** Responsible for path planning, the planner server computes paths based on selected naming conventions and algorithms. These paths are essentially routes calculated on the map.

**② Controller Server:** Also known as the local planner in ROS 1, this server provides the method for the robot to follow the globally computed path or complete local tasks. In simple terms, it controls the robot's movement based on the planned route.

**③ Recovery Server:** The backbone of the fault-tolerant system, the recovery server handles unknown or faulty conditions autonomously. Its goal is to manage unexpected situations the robot may encounter and recover autonomously. For example, if the robot falls into a hole while moving, the recovery server finds a way to get it out.

By continuously switching between planning paths, controlling the robot along the path, and autonomously recovering from issues, the robot achieves autonomous navigation.

During robot navigation, relying solely on the original map generated by SLAM is insufficient. The robot may encounter new obstacles during movement, or it may find that obstacles in certain areas of the original map have disappeared. Therefore, the maintained map during robot navigation is dynamic. Depending on the update frequency and purpose, it can be categorized into the following two types:

① Global Costmap:

The global costmap is mainly used for global path planners. As seen in the structure diagram above, it is located within the Planner Server.

It typically includes the following layers:

Static Map Layer: This layer consists of the static map usually created by SLAM.

Obstacle Map Layer: This layer is used to dynamically record obstacle information perceived by sensors.

Inflation Layer: This layer inflates (expands outward) the maps from the previous two layers to prevent the robot's shell from colliding with obstacles.

② Local Costmap:

The local costmap is primarily used for the local path planner, as seen in the Controller Server in the architecture diagram above. It typically consists of the following layers:

Obstacle Map Layer: This layer records dynamically sensed obstacle information from sensors.

Inflation Layer: This layer inflates (expands outward) on top of the obstacle map layer to prevent the robot's shell from colliding with obstacles.

(2) Feature Pack Installation

The navigation system uses navigation goals, localization information, and map data as inputs to control the robot. It first determines the robot's current location and then identifies the target destination.

You can install the navigation package in two ways:

Use the `apt-get` commands for a quick and easy setup:

```bash
sudo apt install ros-*-navigation2
```

```bash
sudo apt install ros-*-nav2-bringup
```

:::{Note}

Replace \`**\***\` with your ROS 2 version. You can find your ROS 2 version by running \`**echo \$ROS_DISTRO**\` in the terminal.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image6.png"  />

:::



You can install the package in two ways:

① Binary Installation: For a quick and easy setup, you can use binary installation if you are using the navigation package for learning purposes.

② Source Installation: If you need to modify the code to improve algorithms, you will need to download the source code and compile it manually. For detailed instructions on how to do this, please refer to the video tutorial.

You can access the navigation package wiki here: https://wiki.ros.org/Robots/Nav2

:::{Note}

The navigation package is already installed on this robot, so there's no need for additional installation.

:::

### 5.2.2 AMCL Adaptive Monte Carlo Positioning

* **AMCL Positioning**

Positioning involves determining the robot's position in the global map. While SLAM also includes positioning algorithm implementation, SLAM's positioning is utilized for building the global map and occurs before navigation begins. Current positioning, on the other hand, is employed during navigation, where the robot needs to move according to the designated route. Through positioning, it can be assessed whether the robot's actual trajectory aligns with expectations.

The AMCL (Adaptive Monte Carlo Localization) system, provided in the ROS navigation function package ros-navigation, facilitates robot positioning during navigation. AMCL utilizes the adaptive Monte Carlo positioning method and employs particle filters to compute the robot's position based on existing maps.

Positioning addresses the association between the robot and obstacles, as path planning fundamentally involves decision-making based on surrounding obstacles. While theoretically, completing the navigation task only requires knowledge of the robot's global positioning and real-time obstacle avoidance using laser radar and other sensors, the real-time performance and accuracy of global positioning are typically limited. Local positioning from odometry, IMUs, etc., ensures the real-time performance and accuracy of the robot's motion trajectory. The amcl node in the navigation function package provides global positioning by publishing map_odom. However, users can substitute amcl global positioning with other methods that provide map_odom, such as SLAM, UWB, QR code positioning, etc.

Global positioning and local positioning establish a dynamic set of tf coordinates map_odom base_footprint, with static tf coordinates between various sensors in the robot provided through the robot URDF type. This TF relationship resolves the association problem between the robot and obstacles. For instance, if the lidar detects an obstacle 3m ahead, the tf coordinates between the lidar and the robot chassis, base_link to laser_link, are utilized. Through standard transformation, the relationship between the obstacle and the robot chassis can be determined.

* **Particle Filter**

The Monte Carlo positioning process simulates particle updates for a one-dimensional robot. Initially, a group of particles is randomly generated, with each particle representing a potential position, direction, or state variable that requires estimation. Each particle is assigned a weight indicating its similarity to the actual system state. Next, the state of each particle at the next time step is predicted, and particles are moved based on the anticipated behavior of the real system. The weights of the particles are then updated based on measurements. Particles that closely match the measured value receive higher weights and are resampled, while highly unlikely particles are discarded and replaced with more probable ones. Finally, the weighted average and covariance of the particle set are calculated to derive the state estimate.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image7.png"  />

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image8.png"  />

Monte Carlo methods generally follow a specific pattern:

(1) Define possible input fields.

(2) Randomly generate inputs from a probability distribution over the domain.

(3) Perform deterministic calculations on inputs.

(4) Summarize results.

Two important considerations are:

(1) If the points are not uniformly distributed, the approximation effect will be poor.

(2) This process requires many points. The approximation is usually poor if only a few points are randomly placed throughout the square. On average, the accuracy of the approximation improves as more points are placed.

The Monte Carlo particle filter algorithm finds applications in various fields, including physical science, engineering, climatology, and computational biology.

* **Adaptive Monte Carlo Positioning**

AMCL can be viewed as an enhanced iteration of the Monte Carlo positioning algorithm, designed to boost real-time performance by employing a reduced number of samples compared to the traditional method, thereby minimizing execution time. It implements an adaptive or KLD sampling Monte Carlo localization method, utilizing particle filtering to track a robot's pose against an existing map.

The Adaptive Monte Carlo positioning nodes primarily utilize laser scanning and lidar maps to exchange messages and perform pose estimation calculations. The implementation process entails initializing the adaptive Monte Carlo positioning algorithm's particle filter for each parameter provided by the ROS system during initialization. In instances where the initial pose is not specified, the algorithm assumes that the robot commences its journey from the origin of the coordinate system, resulting in a more intricate calculation process.

Hence, it is advisable to set the initial pose using the "**2D Pose Estimate**" button in rviz. For further information on Adaptive Monte Carlo positioning, you can also refer to the wiki address link: **https://github.com/ros-planning/navigation**

* **Cost Map**

Regardless of whether it's a 2D or 3D SLAM map generated by LiDAR or a depth camera, it cannot be directly employed for actual navigation. Instead, it needs to be converted into a costmap. In ROS, the costmap typically adopts a grid format, where each grid cell in the raster map occupies 1 byte (8 bits). This allows for data storage ranging from 0 to 255, representing different cell costs.

The costmap only requires consideration of three scenarios: occupied (barrier), free area (barrier-free), and unknown space.

Before delving into costmap_2d, it's important to introduce the Bresenham algorithm. This algorithm is utilized to draw a straight line between two points by calculating the closest point of a line segment on an n-dimensional raster. It relies solely on relatively fast integer operations such as addition, subtraction, and bit shifting, making it a fundamental algorithm in computer graphics.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image9.png"  />

To construct a set of virtual grid lines, we begin by passing them through the pixel centers of each row and column. The intersection points of these straight lines with each vertical grid line are calculated sequentially, starting from the line's origin and moving towards its endpoint. Subsequently, the pixel closest to each intersection point within the pixel column is determined based on the sign of the error term.

The core concept of the algorithm relies on the assumption that k=dy/dx, where k represents the slope. Since the straight line originates from the center of a pixel, the initial error term d is set to d0＝0. With each increment in the subscript X, d increases by the slope value k, i.e., d=d+k. When d≥1, 1 is subtracted from it to ensure that d remains between 0 and 1. If d≥0.5, the pixel closest to the upper-right of the current pixel is considered (i.e., (x+1,y+1)); otherwise, it's closer to the pixel on the right (i.e.,(x+1,y)). For ease of calculation, let e=d−0.5; initially, e is set to -0.5, and it increments by k. When e≥0, the upper-right pixel of the current pixel (xi, yi) is selected, and when e\<0, the pixel closer to the right (x+1,y) is chosen instead. Integers are preferred to avoid division. Since the algorithm only uses the sign of the error term, it can be replaced as follows: e1 = 2\*e\*dx.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image10.png"   />

The Costmap2D class is responsible for managing the cost value of each raster. Meanwhile, the Layer class serves as a virtual base class, standardizing the interfaces of costmap layers for each plugin. Key interface functions include:

The initialize function, which invokes the onInitialize function to initialize each costmap layer individually.

The matchSize function, found in the StaticLayer and ObstacleLayer classes, ensures consistency across costmap layers by calling the matchSize function of the CostmapLayer class. This initialization process sets the size, resolution, origin, and default cost value of each layer. In the inflationLayer class, a cost table is computed based on the expansion radius, enabling subsequent cost value queries based on distance. Additionally, the seen\_ array is defined to mark traversed rasters. For the VoxelLayer class, initialization involves setting the size of the voxel grid.

The updateBounds function adjusts the size range of the current costmap layer requiring updates. For the StaticLayer class, the update range is determined by the size of the static map (typically used in global costmaps). Conversely, the ObstacleLayer class determines obstacle boundaries by traversing sensor data in clearing_observations.

The initialize and matchSize functions are executed only once each, while updateBounds and updateCosts are performed periodically, their frequency determined by map_update_frequency.

The CostmapLayer class inherits from both the Layer class and the Costmap2D class, providing various methods for updating cost values. The StaticLayer and ObstacleLayer classes, needing to retain cost values of instantiated layers, also inherit from CostmapLayer. The StaticLayer updates its costmap using static raster map data, whereas the ObstacleLayer employs sensor data for updates. In contrast, the VoxelLayer class considers z-axis data more extensively, particularly in obstacle clearance. This distinction primarily affects obstacle removal, with two-dimensional clearance in one case and three-dimensional clearance in the other.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image11.png"   />

Costmap measurement barriers offer remarkable flexibility. You can tailor a specific layer to your requirements, thereby managing pertinent barrier information effectively. For instance, if your robot is equipped solely with LiDAR, you'll need to establish an Obstacles layer to handle obstacle data scanned by the LiDAR. In case ultrasonic sensors are integrated into the robot, creating a new Sonar layer becomes necessary to manage obstacle information from the sonic sensor. Each layer can define its own rules for obstacle updates, encompassing tasks such as obstacle addition, deletion, and confidence level updates. This approach significantly enhances the scalability of the navigation system.

* **Global Path Planning**

Preface: Based on the mobile robot's perception of the environment, the characteristics of the environment, and the algorithms employed, path planning can be categorized into environment-based, map knowledge-based, and completeness-based path planning algorithms.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image12.png"  />

Commonly used path planning algorithms in robot autonomous navigation encompass Diikstra, A\*, D\*, PRM, RRT, genetic algorithms, ant colony algorithms, fuzzy algorithms, and others.

Dijkstra and A\* are graph-based path search algorithms commonly utilized in robotic applications. The navigation function package integrates navfn, global planner, and carrot planner as global route planning plug-ins. Users have the option to select one of these plug-ins to load into move_base for utilization. Alternatively, they can opt for a third-party global path planning plug-in, such as SBPL_Lattice_Planner or srl_global_planner, or develop a custom global path planning plug-in adhering to the interface specification of nav_core.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image13.png"   />

Mobile robot navigation facilitates reaching a target point through path planning. The navigation planning layer comprises several components:

**Global path planning layer**: This layer generates a global weight map based on the provided goal and accepts weight map information. It then plans a global path from the starting point to the target location, serving as a reference for local path planning.

**Local path planning layer**: This component, constituting the local planning aspect of the navigation system, operates on the local weight map information derived from the weight map. It conducts local path planning considering nearby obstacle information.

**Behavior execution layer**: This layer integrates instructions and path planning data from the higher layers to determine the current behavior of the mobile robot.

Path planning algorithms for mobile robots are a crucial area of research, significantly impacting the efficiency of robotic operations.

(1) Dijkstra Algorithm

Dijkstra's algorithm is a classic shortest path algorithm known for its efficiency in finding the shortest path from a single source to all other vertices in a graph. It operates by expanding outward in layers from the starting point, employing a breadth-first search approach while considering edge weights. This makes it one of the most widely used algorithms in global path planning.

Here is a diagram illustrating the process of Dijkstra's algorithm:

① Initially, we set the distance from the starting point (start) to itself as 0, and all other points' distances are initialized to infinity.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image14.png"  />

② In the first iteration, identify the point with the smallest dis value, which is point 1, and mark it as a white point. Next, update the dis values for all blue points connected to point 1, setting dis\[2\] to 2, dis\[3\] to 4, and dis\[4\] to 7.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image15.png"  />

③ In the subsequent iterations, we repeat the process: find the point with the smallest distance value (e.g., Point 2 in the second iteration), mark it as processed, and update the distances of its adjacent points (if necessary). For example, if Point 2 is connected to Points 3 and 5, we update their distances as dis\[3\]=3 and dis\[5\]=4.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image16.png"  />

④ This procedure continues until all points have been processed. At each iteration, we select the unprocessed point with the smallest distance and update distances of its adjacent points accordingly.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image17.png"  />

⑤ Once all points have been processed, the algorithm terminates, and the shortest path distances from the starting point to all other points are determined.

To access the introduction and usage details of the Dijkstra algorithm, please log in to the wiki using the following link : <http://wiki.ros.org/navfn>

(2) A Star Algorithm

A-star is an enhancement of Dijkstra's algorithm tailored for single destination optimization. While Dijkstra's algorithm determines paths to all locations, A-star focuses on finding the path to a specific location or the nearest location among several options. It prioritizes paths that seem to be closer to the goal.

The formula for the A-star algorithm is: *F*=*G*+*H*, where *G* represents the movement cost from the starting point to the designated square, and *H* denotes the estimated cost from the designated square to the endpoint. There are two methods for calculating the *H* value:

① Calculate the distance of horizontal and vertical movement; diagonal calculation is not applicable (Manhattan distance).

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image18.png"  />

② Calculate the distance of horizontal and vertical movement; diagonal calculation is applicable (diagonal distance).

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image19.png"  />

For an introduction to and usage of the A\* algorithm, please consult the video tutorial or visit the following links:

ROS Wiki: [http://wiki.ros.org/global-planner](http://wiki.ros.org/global_planner)

Red Blob Games website: <https://www.redblobgames.com/pathfinding/a-star/introduction.html#graphs>

### 5.2.3 Local Path Planning

Global path planning begins with inputting the starting point and the target point, utilizing obstacle information from the global map to devise a path between them. This path consists of discrete points and solely considers static obstacles. Consequently, while the global path serves as a macro reference for navigation, it cannot be directly utilized for navigation control.

* **DWA Algorithm**

(1) Description

The Dynamic Window Approach (DWA), a classic algorithm for path planning and motion control of mobile robots, ensures safe navigation on a known map. By exploring the speed and angular velocity state space, DWA identifies the optimal combination for safe navigation. Below, you'll find a basic description along with some key formulas of the DWA algorithm.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image20.png"   />

The core concept of the DWA algorithm involves the robot assessing its current state and sensor data to generate a series of potential motion trajectories (referred to as dynamic windows) in the speed and angular velocity state space. These trajectories are then evaluated based on criteria such as obstacle avoidance, maximizing forward speed, and minimizing angular velocity to select the optimal trajectory. Through iterative iterations of this process, the robot dynamically plans its trajectory in real-time to adapt to changing environments and obstacles.

(2) Formula

① Robot status: current position (x, y) and orientation (θ)

② Motion control parameters: linear velocity (V) and angular velocity (ω).

③ Range of velocity and angular velocity sampling: minimum (Vmin, ωmin) and maximum (Vmax, ωmax).

Time step: Δt

The formula is as below:

**Velocity Sampling**: In the DWA algorithm, the initial step involves sampling the state space of velocity and angular velocity to create a set of potential velocity-angular velocity pairs, known as dynamic windows.

V<sub>samples</sub> = \[v<sub>min</sub>, v<sub>max</sub>\]

ω<sub>samples</sub> = \[-ω<sub>max</sub>, ω<sub>max</sub>\]

(V<sub>samples</sub>) denotes the speed sampling range, while (ω<sub>samples</sub>) indicates the angular speed sampling range.

**Motion Simulation:** The DWA algorithm conducts a motion simulation for each speed-angular velocity pair, determining the trajectory of the robot based on these combinations in its current state.

x(t+1) = x(t) + v \* cos(θ(t)) \*Δt

y(t+1) = y(t) + v \* sin(θ(t)) \*Δt

θ(t+1) = θ(t) + ω \* Δt

Here, x(t) and y(t) denote the robot's position, θ(t) represents its orientation, v stands for linear velocity, ω for angular velocity, and Δt represents the time step.

(3) **Trajectory Evaluation:** The DWA algorithm assesses each generated trajectory using evaluation functions, including obstacle avoidance, maximum speed, and minimum angular velocity.

**Obstacle Avoidance Evaluation:** Detects if the trajectory intersects with obstacles.

**Maximum Speed Evaluation:** Verifies if the maximum linear speed on the trajectory falls within the permissible range.

**Minimum Angular Velocity Evaluation:** Ensures that the minimum angular velocity on the trajectory remains within the allowed range.

These evaluation functions can be defined and adjusted as per task requirements.

(4) **Select optimal trajectory**: The DWA algorithm chooses the trajectory with the highest evaluation score as the next move for the robot.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image21.png"   />

* **Expansion**

Extensions and resources for learning about the DWA algorithm:

The DWA algorithm serves as a fundamental method in mobile robotics, with numerous extended and enhanced versions designed to boost performance and efficiency in path planning. Some notable variations include:

(1) DWA Algorithm Extension: <https://arxiv.org/abs/1703.08862>

(2) Enhanced DWA (e-DWA) Algorithm: <https://arxiv.org/abs/1612.07470>)

(3) DP-DWA Algorithm (DP-based Dynamic Window Approach): <https://arxiv.org/abs/1909.05305>

(4) ROS Wiki: http://wiki.ros.org/dwa_local_planner

These resources provide in-depth insights and further exploration into the DWA algorithm and its various extensions.

* **TEB Algorithm**

(1) Description

The TEB (Timed Elastic Band) algorithm is utilized for both path planning and motion planning, particularly in domains like robotics and autonomous vehicles. At its core, the TEB algorithm treats path planning as an optimization challenge, aiming to generate the best trajectory within a specified timeframe while accommodating dynamic constraints and obstacle avoidance needs for the robot or vehicle. Key characteristics of the TEB algorithm encompass:

① **Time Layered Representation:** The TEB algorithm employs time layering, dividing the trajectory into discrete time steps, each corresponding to a position of the robot or vehicle. This aids in setting timing constraints and preventing collisions.

② **Trajectory Parameterization:** TEB parameterizes the trajectory into displacements and velocities, facilitating optimization. Each time step is associated with displacement and velocity parameters.

③ **Constrained Optimization:** TEB considers dynamic constraints, obstacle avoidance, and trajectory smoothness, integrating them into the objective function of the optimization problem.

④ **Optimization Solution:** TEB utilizes techniques like linear quadratic programming (QP) or nonlinear programming (NLP) to determine optimal trajectory parameters that fulfill the constraints.

<p style="margin:0 auto 24px;width:100%">
<img src="../_static/media/chapter_5/section_2/media/image22.png" style="width:48%" />
<img src="../_static/media/chapter_5/section_2/media/image23.png"  style="width:48%;" />
</p>

(2) Formula

The figure below illustrates the optimization objective function in the TEB algorithm:

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image24.png"  />

In the following expression:

J(x) denotes the objective function, where x represents the trajectory parameter.

Wsmooth and Wobstacle represent weights assigned to smoothness and obstacle avoidance, respectively.

H signifies the smoothness penalty matrix.

f(xi, oj) represents the obstacle cost function between trajectory point xi and obstacle oj.

**① Status Definition:**

Firstly, we define the state of the robot (or vehicle) in the path planning problem as follows:

Position: P = \[X, Y\], indicating the coordinates of the robot on the two-dimensional plane.

Velocity: V = \[Vx, Vy\], representing the robot's velocity along the X and Y axes.

Time: t, denoting the current time.

Control Input: u = \[ux, uy\], representing the control input of the robot, which can be speed or acceleration.

Robot Trajectory: x(t) = \[p(t), v(t)\], indicating the state of the robot at time t.

**② Target Function:**

The essence of the TEB algorithm lies in solving an optimization problem. The objective is to minimize a composite function comprising various components:

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image25.png"  />

Jsmooth(x): Smoothness objective function, ensuring trajectory smoothness.

Jobstacle(x): Obstacle avoidance objective function, preventing collisions with obstacles.

Jdynamic(x): Dynamic objective function, ensuring compliance with the robot's dynamic constraints.

**③ Smoothness objective function Jsmooth(x):**

Smoothness objective functions typically involve the curvature of trajectories to ensure the generated trajectories are smooth. It can be represented as:

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image26.png"  />

Where k(t) is the curvature.

**④ Obstacle avoidance objective function Jobstacle(x):**

The obstacle avoidance objective function calculates the distance between trajectory points and obstacles. It penalizes trajectory points that are in close proximity to obstacles. The specific obstacle cost function, denoted as f(x,o), can be adjusted according to specific requirements or needs.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image27.png"  />

**⑤ Dynamic objective function Jdynamic(x):**

The dynamics objective function ensures that the generated trajectory adheres to the robot's dynamic constraints, which are determined by its kinematics and dynamics model. This typically includes limitations on velocity and acceleration.

**⑥ Optimization:**

In the end, the Trajectory Optimization with Ellipsoidal Bounds (TEB) algorithm addresses the stated objective function by framing it as a constrained optimization problem. This problem incorporates optimization variables for trajectory parameters, time allocation, and control inputs. Typically, this optimization problem falls under the category of nonlinear programming (NLP) problems.

* **Expansion**

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image28.png"  />

Additional resources and learning materials for the TEB algorithm:

The TEB algorithm is a significant technology within the realm of path planning, boasting numerous extended and enhanced versions. Below are some learning links and extension topics aimed at facilitating a more comprehensive understanding of the TEB algorithm and its associated concepts:

**(1) Original TEB Paper:** "**Trajectory modification considering dynamic constraints of autonomous robots**" by M. Rösmann et al.

(2) TEB implementation in ROS: The TEB algorithm is commonly implemented as a ROS package (Robot Operating System Package), making it readily available for robot path planning tasks.

ROS TEB Local Planner Package: <https://github.com/rst-tu-dortmund/teb_local_planner>

Wiki website: <http://wiki.ros.org/teb_local_planner>

These links provide valuable resources for users seeking to explore the TEB algorithm and its associated topics in greater detail.

### 5.2.4 Point-to-Point, Multi-Point Navigation and Obstacle Avoidance

For instructions on configuring the connection between the virtual machine and the robot, please refer to "[**5.1 Mapping-\> 5.1.4  slam_toolbox Mapping Algorithm -\> Preparation before Mapping & Navigation**](#anchor_5_1_4)".

* **Robot Operations**

(1) Click-on <img src="../_static/media/chapter_5/section_2/media/image29.png"  /> to start the command line terminal.

(2) Execute the command to disable the app auto-start service:

```bash
~/.stop_ros.sh
```

(3) Enter the following command to start the navigation service and press Enter:

```bash
ros2 launch navigation navigation.launch.py map:=map_01
```

The "**map_01**" at the end of the command is the map name. Users can modify this parameter according to their needs. The map is stored at "**/home/ubuntu/ros2_ws/src/slam/maps**".

* **Virtual Machine Operations**

(1) Click-on <img src="../_static/media/chapter_5/section_2/media/image32.png"  /> to start the virtual machine command-line terminal.

(2) Enter the following command to start the RViz tool for navigation display:

```bash
ros2 launch navigation rviz_navigation.launch.py
```

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image34.png"  />

* **Point-to-Point Navigation**

In the software menu bar, "**2D Pose Estimate**" is used to set the initial position of the JetAutor robot; "**2D Nav Goal**" is used to set a single target point for the robot; "**Publish Point**" is used to set multiple target points for the robot.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image35.png"  />

(1) Click <img src="../_static/media/chapter_5/section_2/media/image36.png"  /> to set the initial position of the robot. On the map interface, choose a position, click, and drag the mouse to select the robot's pose.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image37.png"  />

(2) After setting the initial position of the robot, the effect is as shown in the figure below:

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image38.png"  />

(3) Click <img src="../_static/media/chapter_5/section_2/media/image39.png"  /> and select a location on the map interface as the target point. Click the mouse once at that point. After selection, the robot will automatically generate a route and move to the target point.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image40.png"  />

(4) After confirming the target point, the map will display two paths: the green line represents the direct path between the robot and the target point, and the dark blue line represents the planned path of the robot.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image41.png"  />

(5) When encountering obstacles, the robot will bypass them and continuously adjust its posture and travel route.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image42.png"  />

* **Multi-Point Navigation**

(1) Click the "**waypoint**" button to start multi-point navigation:

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image43.png"  />

(2) Set each navigation point using <img src="../_static/media/chapter_5/section_2/media/image44.png"  /> as shown in the figure below:

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image45.png"  />

(3) Finally, click "**Start Nav Through Poses**" to make the robot move through the set target points one by one.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image46.png"   />

:::{Note}

Clicking "**Start Nav Through Poses**" will not track the robot's pose at each navigation point. "**Start Waypoint Following**" will control the robot to ensure it reaches the pose at each navigation point.

:::

(4) The effect during multi-point navigation is shown in the figure below, with the robot reaching the target points in sequence:

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image47.png"  />

(5) The effect after completing multi-point navigation is shown in the figure below:

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image48.png"  />

* **Launch Description**

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image49.png"   />

The path to the launch file is located at: [/home/ubuntu/ros2_ws/src/navigation/launch/navigation.launch.py](../_static/source_code/navigation.launch.zip)

(1) Set Paths

Locate the paths for the `peripherals`, `controller`, and `servo_controller` packages.

{lineno-start=12}

```python
    if compiled == 'True':
        slam_package_path = get_package_share_directory('slam')
        navigation_package_path = get_package_share_directory('navigation')
    else:
        slam_package_path = '/home/ubuntu/ros2_ws/src/slam'
        navigation_package_path = '/home/ubuntu/ros2_ws/src/navigation'
```

(2) Launch Other Files

`base_launch` for various hardware

`navigation_launch` to start the navigation algorithm

`bringup_launch`  to initialize actions

{lineno-start=34}

```python
    base_launch = IncludeLaunchDescription(
        PythonLaunchDescriptionSource(os.path.join(slam_package_path, 'launch/include/robot.launch.py')),
        launch_arguments={
            'sim': sim,
            'master_name': master_name,
            'robot_name': robot_name
        }.items(),
    )
    
    navigation_launch = IncludeLaunchDescription(
        PythonLaunchDescriptionSource(os.path.join(navigation_package_path, 'launch/include/bringup.launch.py')),
        launch_arguments={
            'use_sim_time': use_sim_time,
            'map': os.path.join(slam_package_path, 'maps', map_name + '.yaml'),
            'params_file': os.path.join(navigation_package_path, 'config', 'nav2_params.yaml'),
            'namespace': robot_name,
            'use_namespace': use_namespace,
            'autostart': 'true',
            'use_teb': use_teb,
        }.items(),
    )

    bringup_launch = GroupAction(
     actions=[
         PushRosNamespace(robot_name),
         base_launch,
         TimerAction(
             period=10.0,  # 延时等待其它节点启动好
             actions=[navigation_launch],
         ),
      ]
    )
```

* **Feature Pack Description**

The path to the navigation package is: **ros2_ws/src/navigation/**

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image53.png"  />

**config:** Contains configuration parameters related to navigation, as shown below:

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image54.png"  />

**launch:** Contains launch files related to navigation, including localization, map loading, navigation modes, and simulation model launch files, as shown below:

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image55.png"  />

**rviz:** Contains parameter loading for the RViz visualization tool, including RViz configuration files for robots using different navigation algorithms and navigation configuration files, as shown below:

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image56.png"  />

**Package.xml:** Configuration information file for the current package.

### 5.2.5 RTAB-VSLAM 3D Navigation

* **Algorithm Introduction & Principle**

For an introduction to the RTAB-VSLAM algorithm and its principles, refer to "[**5.1 Mapping-> 5.1.5 RTAB-VSLAM 3D Vision Mapping & Navigation**](#anchor_5_1_4)" for study and reference.

* **Robot Operations**

(1) Start the robot, and access the robot system desktop using VNC.

(2) Click-on <img src="../_static/media/chapter_5/section_2/media/image29.png"  /> to open the command line terminal.

(3) Execute the command to disable the app auto-start service:

```bash
~/.stop_ros.sh
```

(4) Enter the following command to start the navigation service and press Enter:

```bash
ros2 launch navigation rtabmap_navigation.launch.py
```

* **Virtual Machine Operations**

(1) Click-on <img src="../_static/media/chapter_5/section_2/media/image32.png"  /> to start the virtual-machine command-line terminal.

(2) Run the following command to launch the RVIZ display tool.

```bash
ros2 launch navigation rviz_rtabmap_navigation.launch.py
```

In the software menu bar, "**2D Nav Goal**" is used to set a single target point for the robot, and "**Publish Point**" is used to set multiple target points for the robot.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image59.png"  />

(3) Click <img src="../_static/media/chapter_5/section_2/media/image39.png"  />, and on the map interface, select a location as the target point by clicking the mouse once at that point. After the selection is made, the robot will automatically generate a route and move to the target point.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image60.png"  />

(4) After confirming the target point, the map will display two paths: a line composed of blue squares representing the straight-line path between the robot and the target point, while the dark blue line represents the planned path of the robot.

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image61.png"  />

(5) After encountering obstacles, the car will navigate around them and continuously adjust its posture and trajectory.

*  **Launch Description** 

<img class="common_img" src="../_static/media/chapter_5/section_2/media/image49.png"   />

The launch file is saved in:[/home/ubuntu/ros2_ws/src/navigation/launch/navigation.launch.py](../_static/source_code/navigation.launch.zip)

(1) Set Paths

Locate the paths for the `peripherals`, `controller`, and `servo_controller` packages.

{lineno-start=12}

```python
    if compiled == 'True':
        slam_package_path = get_package_share_directory('slam')
        navigation_package_path = get_package_share_directory('navigation')
    else:
        slam_package_path = '/home/ubuntu/ros2_ws/src/slam'
        navigation_package_path = '/home/ubuntu/ros2_ws/src/navigation'
```

(2) Launch Other Files

`base_launch` for various hardware

`navigation_launch` to start the navigation algorithm

`bringup_launch` to initialize actions

{lineno-start=43}

```python
    navigation_launch = IncludeLaunchDescription(
        PythonLaunchDescriptionSource(os.path.join(navigation_package_path, 'launch/include/bringup.launch.py')),
        launch_arguments={
            'use_sim_time': use_sim_time,
            'map': os.path.join(slam_package_path, 'maps', map_name + '.yaml'),
            'params_file': os.path.join(navigation_package_path, 'config', 'nav2_params.yaml'),
            'namespace': robot_name,
            'use_namespace': use_namespace,
            'autostart': 'true',
            'use_teb': use_teb,
        }.items(),
    )

    bringup_launch = GroupAction(
     actions=[
         PushRosNamespace(robot_name),
         base_launch,
         TimerAction(
             period=10.0,  # 延时等待其它节点启动好
             actions=[navigation_launch],
         ),
      ]
    )

    return [sim_arg, map_name_arg, master_name_arg, robot_name_arg, use_teb_arg, bringup_launch]
```